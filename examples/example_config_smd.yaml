---

s3_config:
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.9.8.72:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.9.8.73:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.9.8.74:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.9.8.75:80
    skipSSLverify: true

# For generating annotations when we start/stop testcases
# https://grafana.com/docs/http_api/annotations/#create-annotation
grafana_config:
  endpoint: http://grafana
  username: admin
  password: grafana

tests:
  - name: write-4k
    # read_weight: 20
    # existing_read_weight: 0
    # write_weight: 80
    # delete_weight: 0
    # list_weight: 0
    write_weight: 100
    objects:
      size_min: 4
      size_max: 4
      # distribution: constant, random, sequential
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 10
      # distribution: constant, random, sequential
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      # distribution: constant, random, sequential
      number_distribution: sequential
    # Name prefix for buckets and objects
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    # End after a set amount of time
    # Runtime in time.Duration - do not forget the unit please
    # stop_with_runtime: 60s # Example with 60 seconds runtime
    stop_with_runtime:
    # End after a set amount of operations (per worker)
    stop_with_ops: 10
    # Number of s3 performance test servers to run in parallel
    workers: 3
    # Set wheter workers share the same buckets or not
    # If set to True - bucket names will have the worker # appended
    workers_share_buckets: True
    # Number of requests processed in parallel by each worker
    parallel_clients: 3
    # Remove all generated buckets and its content after run
    # clean_after: True
    clean_after: False
    write_option:
      # max_upload_parts: 10000
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: read-4k
    existing_read_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 10
      number_distribution: random
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    stop_with_ops: 5
    workers: 3
    workers_share_buckets: True
    parallel_clients: 3
    clean_after: False
    read_option:
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: clean-4k
    delete_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 10
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    stop_with_ops: 10
    skip_prepare: false
    workers: 3
    workers_share_buckets: True
    parallel_clients: 3
    clean_after: True
