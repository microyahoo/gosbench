---
s3_configs:
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.105:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.106:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.107:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.108:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.109:80
    skipSSLverify: true
  - access_key: testy
    secret_key: testy
    region: us-east-1
    endpoint: http://10.16.11.110:80
    skipSSLverify: true

# For generating annotations when we start/stop testcases
# https://grafana.com/docs/http_api/annotations/#create-annotation
grafana_config:
  endpoint: http://grafana
  username: admin
  password: grafana

tests:
  - name: write-4k
    # read_weight: 20
    # existing_read_weight: 0
    # write_weight: 80
    # delete_weight: 0
    # list_weight: 0
    write_weight: 100
    objects:
      size_min: 4
      size_max: 4
      # distribution: constant, random, sequential
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      # distribution: constant, random, sequential
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      # distribution: constant, random, sequential
      number_distribution: sequential
    # Name prefix for buckets and objects
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    # End after a set amount of time
    # Runtime in time.Duration - do not forget the unit please
    # stop_with_runtime: 60s # Example with 60 seconds runtime, may traverse the workqueue multiple times
    # End after a set amount of operations (per worker)
    # stop_with_ops: 10
    # Number of s3 performance test servers to run in parallel
    workers: 6
    # Set wheter workers share the same buckets or not
    # If set to True - bucket names will have the worker # appended
    workers_share_buckets: True
    # Number of requests processed in parallel by each worker
    parallel_clients: 64
    # Remove all generated buckets and its content after run
    # clean_after: True
    clean_after: False
    write_option:
      # max_upload_parts: 10000
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: read-4k
    existing_read_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      number_distribution: random
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    # stop_with_ops: 5
    # stop_with_runtime: 60s # Example with 60 seconds runtime, may traverse the workqueue multiple times
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: False
    read_option:
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: clean-4k
    delete_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-
    object_prefix: obj-
    # stop_with_ops: 10
    skip_prepare: true
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: True
  - name: write-64k
    # read_weight: 20
    # existing_read_weight: 0
    # write_weight: 80
    # delete_weight: 0
    # list_weight: 0
    write_weight: 100
    objects:
      size_min: 64
      size_max: 64
      # distribution: constant, random, sequential
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      # distribution: constant, random, sequential
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      # distribution: constant, random, sequential
      number_distribution: sequential
    # Name prefix for buckets and objects
    bucket_prefix: gosbench-prefix-64k-
    object_prefix: obj-64k-
    # End after a set amount of time
    # Runtime in time.Duration - do not forget the unit please
    # stop_with_runtime: 60s # Example with 60 seconds runtime, may traverse the workqueue multiple times
    # End after a set amount of operations (per worker)
    # stop_with_ops: 10
    # Number of s3 performance test servers to run in parallel
    workers: 6
    # Set wheter workers share the same buckets or not
    # If set to True - bucket names will have the worker # appended
    workers_share_buckets: True
    # Number of requests processed in parallel by each worker
    parallel_clients: 64
    # Remove all generated buckets and its content after run
    # clean_after: True
    clean_after: False
    write_option:
      # max_upload_parts: 10000
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: read-64k
    existing_read_weight: 100
    objects:
      size_min: 64
      size_max: 64
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      number_distribution: random
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-64k-
    object_prefix: obj-64k-
    # stop_with_ops: 5
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: False
    read_option:
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: clean-64k
    delete_weight: 100
    objects:
      size_min: 64
      size_max: 64
      size_distribution: constant
      unit: KB
      number_min: 1
      number_max: 1000000
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-64k-
    object_prefix: obj-64k-
    # stop_with_ops: 10
    skip_prepare: true
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: True
  - name: write-4M
    # read_weight: 20
    # existing_read_weight: 0
    # write_weight: 80
    # delete_weight: 0
    # list_weight: 0
    write_weight: 100
    objects:
      size_min: 4
      size_max: 4
      # distribution: constant, random, sequential
      size_distribution: constant
      unit: MB
      number_min: 1
      number_max: 1000000
      # distribution: constant, random, sequential
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      # distribution: constant, random, sequential
      number_distribution: sequential
    # Name prefix for buckets and objects
    bucket_prefix: gosbench-prefix-4M-
    object_prefix: obj-4M-
    # End after a set amount of time
    # Runtime in time.Duration - do not forget the unit please
    # stop_with_runtime: 60s # Example with 60 seconds runtime, may traverse the workqueue multiple times
    # End after a set amount of operations (per worker)
    # stop_with_ops: 10
    # Number of s3 performance test servers to run in parallel
    workers: 6
    # Set wheter workers share the same buckets or not
    # If set to True - bucket names will have the worker # appended
    workers_share_buckets: True
    # Number of requests processed in parallel by each worker
    parallel_clients: 64
    # Remove all generated buckets and its content after run
    # clean_after: True
    clean_after: False
    write_option:
      # max_upload_parts: 10000
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: read-4M
    existing_read_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: MB
      number_min: 1
      number_max: 1000000
      number_distribution: random
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-4M-
    object_prefix: obj-4M-
    # stop_with_ops: 5
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: False
    read_option:
      # concurrency: 4
      # chunk_size: 40
      # unit: MB
  - name: clean-4M
    delete_weight: 100
    objects:
      size_min: 4
      size_max: 4
      size_distribution: constant
      unit: MB
      number_min: 1
      number_max: 1000000
      number_distribution: sequential
    buckets:
      number_min: 1
      number_max: 10
      number_distribution: sequential
    bucket_prefix: gosbench-prefix-4M-
    object_prefix: obj-4M-
    # stop_with_ops: 10
    skip_prepare: true
    workers: 6
    workers_share_buckets: True
    parallel_clients: 64
    clean_after: True
